{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data preparation and model selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression, ElasticNet\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import NuSVR\n",
    "from numpy import random\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import NuSVR\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from smogn import smoter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "mlflow.set_experiment(\"Obsesity Regression\")\n",
    "\n",
    "#TODO Deep learning model with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = pd.read_csv(\"data/X1.csv\", index_col=0)\n",
    "Y1 = pd.read_csv(\"data/Y1.csv\", header=None, names=['Weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataEncoder(BaseEstimator):\n",
    "    def __init__(self): # no *args or **kargs\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        columns = ['Gender', 'Age', 'Height', 'Family history with overweight', 'Consumption of high caloric food',\n",
    "           'Consumption frequency of vegetables', 'Number of main meals daily', 'Food Consumption between meals', 'Smoke',\n",
    "           'Water consumption daily', 'Calories consumption monitoring', 'Physical activity frequency', 'Time using technology devices',\n",
    "           'Alcohol consumption', 'Usual transportation used']\n",
    "        X.columns = columns\n",
    "        X['Consumption frequency of vegetables'] = X['Consumption frequency of vegetables'].replace(\n",
    "            {1: '1. Never', 2: '2. Sometimes', 3: '3. Always'})\n",
    "        X['Number of main meals daily'] = X['Number of main meals daily'].replace(\n",
    "            {1: '1', 2: '2', 3: '3', 4: '3+'})\n",
    "        X['Water consumption daily'] = X['Water consumption daily'].replace(\n",
    "            {1: '1. Less than 1L', 2: '2. Between 1L and 2L', 3: '3. More than 2L'})\n",
    "        X['Physical activity frequency'] = X['Physical activity frequency'].replace(\n",
    "            {0: '1. I do not have', 1: '2. 1 or 2 days', 2: '3. 2 or 4 days', 3: '4. 4 or 5 days'})\n",
    "        X['Time using technology devices'] = X['Time using technology devices'].replace(\n",
    "            {0: '1. 0–2 hours', 1: '2. 3–5 hours', 2: '3. More than 5 hours'})\n",
    "        \n",
    "        ordinal_cat = ['Consumption frequency of vegetables', 'Number of main meals daily', 'Food Consumption between meals',\n",
    "               'Water consumption daily', 'Physical activity frequency', 'Time using technology devices',\n",
    "               'Alcohol consumption']\n",
    "        for cat in ordinal_cat:\n",
    "            X[cat] = X[cat].map(lambda x: sorted(X[cat].unique()).index(x)+1)\n",
    "\n",
    "        # Handling binary categorical features\n",
    "        X[['Family history with overweight', 'Consumption of high caloric food', 'Smoke', 'Calories consumption monitoring']] = X[[\n",
    "            'Family history with overweight', 'Consumption of high caloric food', 'Smoke', 'Calories consumption monitoring']].replace(to_replace=['no', 'yes'], value=[0, 1])\n",
    "        X[['Gender']] = X[['Gender']].replace(\n",
    "            to_replace=['Female', 'Male'], value=[0, 1])\n",
    "        X['Age'] = X['Age'].map(lambda x: np.log(x))\n",
    "\n",
    "        return pd.get_dummies(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(height, weight):\n",
    "    bmi = weight/height**2\n",
    "    if 0 < bmi < 18.5:\n",
    "        return 1\n",
    "    if 18.5 < bmi < 25.0:\n",
    "        return 2\n",
    "    if 25.0 < bmi < 30.0:\n",
    "        return 3\n",
    "    if bmi >=30.0:\n",
    "        return 4\n",
    "\n",
    "df = pd.concat([X1,Y1], axis=1)\n",
    "labels = df.apply(lambda x : get_label(x['Height'],x['Weight']),axis=1)\n",
    "y = Y1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the outliers\n",
    "for col in ['Age', 'Height', 'Weight']:\n",
    "    upper_lim = df[col].quantile(.95)\n",
    "    lower_lim = df[col].quantile(.05)\n",
    "\n",
    "    df.loc[(df[col] > upper_lim), col] = np.nan\n",
    "    df.loc[(df[col] < lower_lim), col] = np.nan\n",
    "\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating pipelines and scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling all numerical features without the target\n",
    "\n",
    "#TODO pipeline with the 3 data augmentation methods\n",
    "pipeline = Pipeline(steps=[('Encoder', DataEncoder()),('Scaler', StandardScaler())])\n",
    "# pipeline_cat_smote = None\n",
    "# pipeline_cat_border = None\n",
    "# pipeline_cat_asdyn = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def score_weight_class(bmi_pred, bmi_true, low, high):\n",
    "#     tol = 1\n",
    "#     vpred = (bmi_pred >= low - tol) & (bmi_pred < high+tol)\n",
    "#     vtrue = (bmi_true >= low) & (bmi_true < high)\n",
    "#     if vtrue.sum() == 0:\n",
    "#         print(\"no true samples here\")\n",
    "#         return 0\n",
    "#     rmse = np.sqrt(((bmi_true[vtrue]-bmi_pred[vtrue])**2).mean())\n",
    "#     rmse = rmse/(high-low+tol)  # normalize rmse in interval\n",
    "#     acc = (vpred & vtrue).sum()/vtrue.sum()\n",
    "#     return rmse*(1-acc)\n",
    "\n",
    "\n",
    "# def score_regression(ytrue, ypred, height):\n",
    "#     bmi_pred = ypred/(height*height)\n",
    "#     bmi_true = ytrue/(height*height)\n",
    "#     scores = []\n",
    "#     for bmi_low, bmi_high in zip([0, 18.5, 25, 30], [18.5, 25, 30, 100]):\n",
    "#         scores.append(score_weight_class(bmi_pred, bmi_true,\n",
    "#                                          low=bmi_low, high=bmi_high))\n",
    "#     return np.mean(scores)\n",
    "\n",
    "# def _score(estimator, X, y):\n",
    "#     ypred = estimator.predict_proba(X)\n",
    "#     print(ypred)\n",
    "#     return score_regression(y, ypred, X[6])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSmoter(BaseEstimator):\n",
    "    def __init__(self,**kwargs): # no *args or **kargs\n",
    "        self.kwargs = kwargs\n",
    "    def fit(self, X, y=None):\n",
    "        return self # nothing else to do\n",
    "    def transform(self, X, y=None):\n",
    "        X.reset_index(drop=True,inplace=True)\n",
    "        y.reset_index(drop=True,inplace=True)\n",
    "        df = pd.concat([X,y], axis=1)\n",
    "        df_smoter = smoter(\n",
    "        data = df, \n",
    "        y = \"Weight\",\n",
    "        samp_method = 'extreme',\n",
    "        **self.kwargs\n",
    "        )\n",
    "        df_smoter.drop_duplicates(inplace=True)\n",
    "        return df_smoter.drop(\"Weight\",axis=1), df_smoter['Weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Ensemble methods https://scikit-learn.org/stable/modules/ensemble.html\n",
    "models = {\n",
    "    'Lasso': Lasso(),\n",
    "    'Ridge': Ridge(),\n",
    "    'KNeighborsRegressor': KNeighborsRegressor(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'NuSVR': NuSVR()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'Lasso' : {\n",
    "        'alpha': random.uniform(low=0.001, high=15, size=100),\n",
    "    },\n",
    "    'Ridge' : {\n",
    "        'alpha': random.uniform(low=10, high=50, size=100),\n",
    "    },\n",
    "    'KNeighborsRegressor' : {\n",
    "        'weights': ['uniform', 'distance'],\n",
    "        'n_neighbors': random.randint(20, 100, size=100),\n",
    "    },\n",
    "    'ElasticNet' : {\n",
    "        'alpha':  random.uniform(low=0.001, high=2, size=100),\n",
    "        'l1_ratio': random.uniform(low=0.001, high=1, size=100),\n",
    "    },\n",
    "    'NuSVR' : {\n",
    "        'nu': random.uniform(low=0.001, high=1, size=100),\n",
    "        'C': random.uniform(low=0.01, high=20, size=100),\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "}\n",
    "\n",
    "X = DataEncoder().transform(X1)\n",
    "y = Y1\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, shuffle=True,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RandomizedSearchCV for Lasso.\n",
      "Fitting 6 folds for each of 100 candidates, totalling 600 fits\n",
      "Running RandomizedSearchCV for Ridge.\n",
      "Fitting 6 folds for each of 100 candidates, totalling 600 fits\n",
      "Running RandomizedSearchCV for KNeighborsRegressor.\n",
      "Fitting 6 folds for each of 100 candidates, totalling 600 fits\n",
      "Running RandomizedSearchCV for ElasticNet.\n",
      "Fitting 6 folds for each of 100 candidates, totalling 600 fits\n",
      "Running RandomizedSearchCV for NuSVR.\n",
      "Fitting 6 folds for each of 100 candidates, totalling 600 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marci\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:63: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "cv = 6\n",
    "for key in models.keys():\n",
    "    with mlflow.start_run():\n",
    "        print(\"Running RandomizedSearchCV for %s.\" % key)\n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        k=math.ceil(len(train_y) * 0.02)\n",
    "        pipeline = Pipeline(steps=[('smoter', DataSmoter(random_state=42,k=k,pert=0.04)),('Scaler', StandardScaler()),(key,model)])\n",
    "        gs = RandomizedSearchCV(model, param, n_iter=100, cv=cv, n_jobs=-2,\n",
    "                                verbose=8, scoring='neg_root_mean_squared_error', refit='neg_root_mean_squared_error',\n",
    "                                )\n",
    "        gs.fit(train_x, train_y)\n",
    "        mlflow.log_param(\"Regressor\", key)\n",
    "        mlflow.log_metric(\"score_train\", float(gs.best_score_))\n",
    "        mlflow.log_metric(\"CV\", cv)\n",
    "        preds = gs.predict(test_x)\n",
    "        mlflow.log_metric(\"score_test_rmse\", mean_squared_error(preds,test_y,squared=False))\n",
    "        for key in gs.best_params_:\n",
    "            mlflow.log_param(key, gs.best_params_[key])\n",
    "        mlflow.sklearn.log_model(gs.best_estimator_, \"best_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.ceil(len(train_y) * 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def eval(predict, target):\n",
    "\n",
    "#     rmse = np.sqrt(mean_squared_error(predict, target))\n",
    "#     mae = mean_absolute_error(predict, target)\n",
    "#     r2 = r2_score(predict, target)\n",
    "#     return rmse, mae, r2\n",
    "\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     X, y, random_state=42, test_size=0.2)\n",
    "\n",
    "# lg = Lasso(alpha=0.2)\n",
    "# lg.fit(X_train, y_train)\n",
    "# preds = lg.predict(X_test)\n",
    "\n",
    "# print(score_regression(y_test,preds,X_test[6].map(lambda x : x/100)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Model selection (Classification)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_class = df.copy()\n",
    "\n",
    "\n",
    "def get_label(height, weight):\n",
    "    bmi = weight/height**2\n",
    "    if 0 < bmi < 18.5:\n",
    "        return 1\n",
    "    if 18.5 < bmi < 25.0:\n",
    "        return 2\n",
    "    if 25.0 < bmi < 30.0:\n",
    "        return 3\n",
    "    if bmi >=30.0:\n",
    "        return 4\n",
    "\n",
    "df_class['label'] = df_class.apply(lambda x : get_label(x['Height'],x['Weight']),axis=1)\n",
    "df_class.drop('Weight', inplace=True,axis=1)\n",
    "\n",
    "\n",
    "X = pd.get_dummies(df_class.drop('label', axis=1))\n",
    "y = df_class['label']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"Obsesity Classification\")\n",
    "\n",
    "# TODO Ensemble methods https://scikit-learn.org/stable/modules/ensemble.html\n",
    "models = {\n",
    "    'RandomForestClassifier' : RandomForestClassifier()\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'RandomForestClassifier': {\n",
    "        'n_estimators': random.randint(1, 1000, 50),\n",
    "        'max_depth': random.randint(1, 1000, 50),\n",
    "        'max_features': ['sqrt', 'log2', 'auto'],\n",
    "        'min_samples_split': random.randint(1, 1000, 50),\n",
    "        'min_samples_leaf': random.randint(1, 1000, 50),\n",
    "    }\n",
    "}\n",
    "\n",
    "# X = pd.DataFrame(pipeline.fit_transform(df_class)).drop(18, axis=1)\n",
    "# y = df_class['label']\n",
    "# y = y.astype(int)\n",
    "X\n",
    "cv = 5\n",
    "for key in models.keys():\n",
    "    with mlflow.start_run():\n",
    "        print(\"Running RandomizedSearchCV for %s.\" % key)\n",
    "        model = models[key]\n",
    "        param = params[key]\n",
    "        gs = RandomizedSearchCV(model, param, n_iter=50, cv=cv, n_jobs=-2,\n",
    "                                verbose=3, scoring='accuracy'\n",
    "                                )\n",
    "        gs.fit(X_resampled, y_resampled)\n",
    "        mlflow.log_param(\"Classifier\", key)\n",
    "        mlflow.log_metric(\"best_score\", float(gs.best_score_))\n",
    "        mlflow.log_metric(\"CV\", cv)\n",
    "        for key in gs.best_params_:\n",
    "            mlflow.log_param(key, gs.best_params_[key])\n",
    "        mlflow.sklearn.log_model(gs.best_estimator_, \"best_model\")\n",
    "# def eval(predict, target):\n",
    "\n",
    "#     rmse = np.sqrt(mean_squared_error(predict, target))\n",
    "#     mae = mean_absolute_error(predict, target)\n",
    "#     r2 = r2_score(predict, target)\n",
    "#     return rmse, mae, r2\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42, test_size=0.1)\n",
    "\n",
    "lg = KNeighborsClassifier()\n",
    "lg.fit(X_train, y_train)\n",
    "preds = lg.predict(X_test)\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, preds, labels=[1,2,3,4])\n",
    "\n",
    "sns.heatmap(conf_matrix,annot=True,fmt='g',xticklabels=['underweight','normal','overweight','obese'],yticklabels=['underweight','normal','overweight','obese'])\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "643983cb912722448ab70204d5f58fc7d67fc5fc0e3de367d07cf898678346f4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
